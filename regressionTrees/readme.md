# CART算法和树剪枝技术。

## 将CART（Classification And Regression Trees）算法用于回归
在之前，我们学习了决策树的原理和代码实现，使用使用决策树进行分类。决策树不断将数据切分成小数据集，直到所有目标标量完全相同，或者数据不能再切分为止。决策树是一种贪心算法，它要在给定时间内做出最佳选择，但不关心能否达到全局最优。

### ID3算法的弊端
回忆一下，决策树的树构建算法是ID3。ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有4种取值，那么数据将被切分成4份。一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。

除了切分过于迅速外，ID3算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征离散化，才能在ID3算法中使用。但这种转换过程会破坏连续型变量的内在特性。

### CART算法
假设X与Y分别为输入和输出变量，并且Y是连续变量，给定训练数据集：
	D = {(x1, y1),(x2, y2), ...{xn, yn}}
其中，D表示整个数据集合，n为特征数。
一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为M个单元R1,R2,...Rm，并且在每个单元Rm上有一个固定的输出值Cm，于是回归树模型可表示为：
	f(x) = M∑ CmI(x∈Rm)
这样就可以计算模型输出值与实际值的误差：
	∑(xi∈Rm)(yi - f(xi))2
我们希望每个单元上的Cm，可以是的这个平方误差最小化。易知，当Cm为相应单元的所有实际值的均值时，可以到最优：
	Cm~ = ave(yi|xi∈Rm)
那么如何生成这些单元划分？
假设，我们选择变量 xj 为切分变量，它的取值 s 为切分点，那么就会得到两个区域：
	R1(j, s) = {x|x(j) <= s}, R2(j,s) = {x|x(j) > s}
当j和s固定时，我们要找到两个区域的代表值c1，c2使各自区间上的平方差最小

那么对固定的 j 只需要找到最优的s，然后通过遍历所有的变量，我们可以找到最优的j，这样我们就可以得到最优对（j，s），并得到两个区间。
这样的回归树通常称为最小二乘回归树（least squares regression tree）。

与ID3算法相反，CART算法正好适用于连续型特征。CART算法使用二元切分法来处理连续型变量。而使用二元切分法则易于对树构建过程进行调整以处理连续型特征。具体的处理方法是：如果特征值大于给定值就走左子树，否则就走右子树。

CART算法有两步：

决策树生成：递归地构建二叉决策树的过程，基于训练数据集生成决策树，生成的决策树要尽量大；自上而下从根开始建立节点，在每个节点处要选择一个最好的属性来分裂，使得子节点中的训练集尽量的纯。不同的算法使用不同的指标来定义"最好"：
决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。
决策树剪枝我们先不管，我们看下决策树生成。

在决策树的文章中，我们先根据信息熵的计算找到最佳特征切分数据集构建决策树。CART算法的决策树生成也是如此，实现过程如下：

使用CART算法选择特征
根据特征切分数据集合
构建树


## 树剪枝
一棵树如果结点过多，表明该模型可能对数据进行了“过拟合”。

通过降低树的复杂度来避免过拟合的过程称为剪枝（pruning）。上小节我们也已经提到，设置tolS和tolN就是一种预剪枝操作。另一种形式的剪枝需要使用测试集和训练集，称作后剪枝（postpruning）。本节将分析后剪枝的有效性，但首先来看一下预剪枝的不足之处。

后剪枝
使用后剪枝方法需要将数据集分成测试集和训练集。首先指定参数，使得构建出的树足够大、足够复杂，便于剪枝。接下来从上而下找到叶结点，用测试集来判断这些叶结点合并是否能降低测试集误差。如果是的话就合并。

## 总结
CART算法可以用于构建二元树并处理离散型或连续型数据的切分。若使用不同的误差准则，就可以通过CART算法构建模型树和回归树。
一颗过拟合的树常常十分复杂，剪枝技术的出现就是为了解决这个问题。两种剪枝方法分别是预剪枝和后剪枝，预剪枝更有效但需要用户定义一些参数。